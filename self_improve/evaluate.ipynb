{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d03b3eb-37d3-4666-9257-9fe9c3282f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1f1b7340-d7e7-4631-9197-d0d54e5a4ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_val = 0\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5c9cc8-7836-495b-8702-366af489dc60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-417f2549269fa1f4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeb289ecab64c119d674bdea82ff5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=\"/root/autodl-tmp/BIG-Bench-Hard/bbh/web_of_lies.json\",field=\"examples\")['train']\n",
    "ds_split=ds.train_test_split(test_size=0.2)\n",
    "prompt_all=ds['input']\n",
    "prompt_all_new= [prompt.replace('\\n', ' ') for prompt in prompt_all]\n",
    "prompt_all_new=['[{}] Let’ s think step by step.'.format(prompt.replace('\\n', ' ')) for prompt in prompt_all_new]\n",
    "answer_all=ds['target']\n",
    "prompt_train=ds_split['train']['input']\n",
    "prompt_test=ds_split['test']['input']\n",
    "answer_test=ds_split['test']['target']\n",
    "answer_train=ds_split['train']['target']\n",
    "\n",
    "\n",
    "\n",
    "prompt_test_new= ['[{}] Let’ s think step by step.'.format(prompt.replace('\\n', ' ')) for prompt in prompt_test]\n",
    "prompt_train_new= ['[{}] Let’ s think step by step.'.format(prompt.replace('\\n', ' ')) for prompt in prompt_train]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def accuracy(prompts: List[str], outputs: List[str]) -> Dict[str, List[float]]:\n",
    "    match=[]\n",
    "        \n",
    "    for i,prompt in enumerate(prompts):\n",
    "\n",
    "        index = prompt_all_new.index(prompt)\n",
    "        if answer_all[index].lower().strip() in outputs[i].lower().strip():\n",
    "            is_correct=1.0\n",
    "        else:\n",
    "            is_correct=0.0\n",
    "                \n",
    "        match.append(is_correct)\n",
    "\n",
    "    return sum(match)/len(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e49707-38cf-421c-b7d2-45d49f0d066d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # Load the model\n",
    "model_se_0 = T5ForConditionalGeneration.from_pretrained(\"/root/autodl-tmp/flan-t5-large\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "tokenizer_se = T5Tokenizer.from_pretrained(\"/root/autodl-tmp/flan-t5-large\")\n",
    "\n",
    "    # Create the pipeline\n",
    "se_generator= pipeline(\"text2text-generation\", model=model_se_0, tokenizer=tokenizer_se,\n",
    "                        do_sample= True,\n",
    "                        top_k= 10,\n",
    "                        top_p= 0.99,\n",
    "                        max_length=100,\n",
    "                        eos_token_id= tokenizer_se.eos_token_id,\n",
    "                        temperature= 1.0,\n",
    "                          device=0 if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0 else -1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "985dce24-53ee-4995-a66c-148990fce11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/output_model/web_of_lies_01epoch_5 were not used when initializing T5ForConditionalGeneration: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "    # Load the model\n",
    "model_se_1 = T5ForConditionalGeneration.from_pretrained(\"/root/autodl-tmp/output_model/web_of_lies_01epoch_5\")\n",
    "\n",
    "    # Create the pipeline\n",
    "se_generator_1 = pipeline(\"text2text-generation\", model=model_se_1, tokenizer=tokenizer_se,\n",
    "                        do_sample= True,\n",
    "                        top_k= 10,\n",
    "                        top_p= 0.99,\n",
    "                        max_length=100,\n",
    "                        eos_token_id= tokenizer_se.eos_token_id,\n",
    "                        temperature= 1.0,\n",
    "                        device=0 if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0 else -1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "764887db-a487-45a4-b06c-67ca2a904ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs_0=[]\n",
    "for i in range(len(prompt_train)):\n",
    "    \n",
    "    question=prompt_train[i]\n",
    "    answer=answer_train[i]\n",
    "    #print('££££££££££££££££££££££££££',question)\n",
    "    generation=se_generator(question)[0]['generated_text']\n",
    "    outputs_0.append(generation)\n",
    "    #print('$$$$$$$$$$$$$',generation)\n",
    "    #print('============',answer)\n",
    "    #print(reward_fn(prompts=[question], outputs=[generation]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80df51be-5ef1-4d41-ab85-5132e689d441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs_1=[]\n",
    "for i in range(len(prompt_train)):\n",
    "    \n",
    "    question=prompt_train[i]\n",
    "    answer=answer_train[i]\n",
    "    # print('££££££££££££££££££££££££££',question)\n",
    "    generation=se_generator_1(question)[0]['generated_text']\n",
    "    outputs_1.append(generation)\n",
    "    # print('$$$$$$$$$$$$$',generation)\n",
    "    #print('============',answer)\n",
    "    #print(reward_fn(prompts=[question], outputs=[generation]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aea18414-d990-4b6e-bd90-d5bcabbe49e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495\n"
     ]
    }
   ],
   "source": [
    "accuracy_0=accuracy(prompt_train_new,outputs_0)\n",
    "print(accuracy_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dca7bc8-118d-4b00-b5a7-6b21c82158cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n"
     ]
    }
   ],
   "source": [
    "accuracy_1=accuracy(prompt_train_new,outputs_1)\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48d9b4-cc2d-4dce-8ee2-eb3ef1626cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
